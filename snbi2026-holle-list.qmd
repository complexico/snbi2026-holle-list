---
title: "The Digitised Holle List Project: Building Database from Legacy Materials for Conserving Indigenous Indonesian Languages"
title-prefix: "Digitsed Holle List"
author: 
  - name:
      given: "Gede Primahadi Wijaya"
      family: "Rajeg"
    orcid: "0000-0002-2047-8621"
    affiliation: 
      id: unud
      name: Universitas Udayana
      country: Indonesia
date: 2026-02-03
date-modified: now
abstract: 'Advances in cloud computing, as well as computational tools for extracting text from images, offer an opportunity to scale up the development of digital databases for Indigenous languages. This paper reports on the application of these advances to the digitalisation of old, paper-borne lexical items of over a hundred Indigenous languages in Indonesia; these items are part of the so-called [Holle List (HL)](http://hdl.handle.net/1885/144430). After introducing the (structure of the) HL, the paper underlines the motivation for the [HL digitalisation project](https://portal.sds.ox.ac.uk/projects/Digitised_Holle_List/259172). It then provides an overview of *[Google Colab](https://colab.research.google.com/)* as a free cloud-computing platform for executing a series of optical character recognition (OCR) operations on hundreds of scanned pages of the HL, utilising *[pytesseract](https://pypi.org/project/pytesseract/)*, a Python interface for *[Google’s Tesseract-OCR engine](https://github.com/tesseract-ocr/tesseract)*. Advantages (e.g., computational searchability and manipulability), as well as issues (especially typos and unrecognised characters) in the plain-text OCR outputs, are discussed. In conclusion, the paper highlights the importance of digital technology in conserving Indigenous languages via digital platforms, albeit some unavoidable challenges that require humans (linguistic and manual) intervention.'
format:
    html: 
      page-layout: full
      number-sections: true
      fig-width: 6
      fig-asp: 0.618
      fig-dpi: 300
      code-fold: true
      citations-hover: true
      footnotes-hover: true
      toc: true
      toc-location: left
      crossref: 
        sec-prefix: §
        
    docx:
      toc: false
      number-sections: true
      highlight-style: github
      fig-width: 6
      fig-asp: 0.618
      fig-dpi: 300
      echo: false
      crossref:
        sec-prefix: §
editor: visual
bibliography: references.bib
csl: apa7.csl
google-scholar: true
search: true
---

## Introduction {#sec-intro}

This paper reports on a Digital Humanities [@drucker2021] project of digitalising and curating large volumes of word list, the so-called *Holle List vocabulary*. The Holle List project was initiated in the late 19^th^ century by [Karel Frederik Holle](https://en.wikipedia.org/wiki/Karel_Holle), a Dutch colonial administrator. His aim was to gain knowledge about the linguistic situation of the Dutch East Indies of the present-day state of Indonesia. In the first edition of the Holle List [@holle1894], K. F. Holle set up a list of elicitation concepts (i.e., 905 concepts to be exact) given in Dutch [@holle1894, 8-38]. This list was dispersed throughout the Indonesian archipelago. The goal was to collect the corresponding expressions/words of these elicited concepts (from various semantic domains) across over two hundreds indigenous regional language-varieties of Indonesia.

Between 1980 and 1987, W. A. L. Stokhof and colleagues (viz. Lia Saleh-Bronckhorst and Alma E. Almanar) edited, collated, and published (i) the different versions of the reference/master, elicitation Holle List as well as (ii) the corresponding expressions/words in the indigenous language-varieties into eleven-volume publication series[^1]. These publications are available open access under the Creative Commons License (see @fig-holle-list-search-output in @sec-data-source-acquisition). There are two main parts of these publications. The first one is the volume containing just the reference (or master) Holle List [@holleli1980], comprising the elicitation concepts given in Dutch, English, and Indonesian/Malay together with the index numbers of these concepts (see @fig-matching-master-and-regional-1); this is called *The New Basic List* (hereafter NBL) in Stokhof [-@holleli1980] because Stokhof and colleagues collated three different versions of the Holle List (namely those published in 1894, 1904/1911, and 1931; see @fig-matching-master-and-regional-1). The second part of the Holle List publications is the separate volumes containing the expressions/words of the regional language-varieties and their index numbers (see @fig-matching-master-and-regional-2 for an example from the Enggano language); these index numbers for words in the regional languages correspond to the index numbers of the concepts in the reference Holle List/NBL. It is important to note that there is only one volume of the NBL; the content of the NBL is not repeated in the remaining volumes for the expressions/words in the regional language-varieties, but only the index numbers.

[^1]: See these Holle List publication series [on this page](https://openresearch-repository.anu.edu.au/search?spc.page=1&query=Holle%20Lists&scope=bbe2244b-212d-44b4-b33a-59048fd7e13f).

In the above two-parts publication setup, linguists, who are interested in the Dutch, English, and Indonesian/Malay translations of *a* given word in *a* given regional language, must manually match the index number of that regional word with the same index number in the reference Holle List. Let us use the data snippet in @fig-matching-master-and-regional as an example.

```{r message=FALSE, warning=FALSE}
#| label: fig-matching-master-and-regional
#| fig-cap: "Correspondence between index numbers of the regional list of Enggano and of the reference Holle List (or the New Basic List [NBL])."
#| fig-subcap: 
#|   - "the reference Holle List/NBL [@holleli1980]"
#|   - "Enggano regional list [@holle-list-barrier-islands]"
#| fig-cap-location: bottom
#| out-width: "80%"
#| layout-nrow: 2

library(tidyverse)

knitr::include_graphics("img/matching-master-list.png")
knitr::include_graphics("img/matching-enggano-list.png")

```

Consider the Enggano word “èbaka” (ID number 3 in @fig-matching-master-and-regional-2). To understand what the word refers to in Dutch, English, and Indonesian/Malay, one must look up the ID number 3 in the separate NBL publication (@fig-matching-master-and-regional-1). In this case, “èbaka” in Enggano refers to ‘*gezicht, aangezicht*’ in Dutch, ‘face’ in English, and ‘*muka*, *wajah*’ in Indonesian/Malay. Alternatively, from the perspective of the NBL, linguists could have asked how a given concept is lexicalised in a given language. For example, the concept of ‘*lichaam*’ or ‘body’ in English (and ‘*badan, tubuh*’ in Indonesian) (ID number 1 in the NBL) can be lexicalised by two forms in Enggano, as shown by those given for the ID number 1 in the Enggano list, viz. “kărāhā” and “koedŏdŏkŏ” (cf. @tbl-land and @tbl-borneo-land).

With such paper-based, separate setup between words in the regional languages and their translations, one could imagine the amount of manual back-and-forth procedure needed to link the words and their translations. The development of modern data science [@donoho2017] allows us to handle such a problem in a computational way. The Holle List setup can be conceived as disjointed relational data with common keys; these shared keys are the index numbers present in both data set (the NBL and the given regional list). Then, they can be computationally joined at scale once they are both in computer-readable format [see @wickham2023, Ch. 19, for the description of table-joining and its computational implementation in the R programming language].

In order to tackle the problem of manual matching, with a desiderata for computational matching between the NBL and the regional lists, the PDF file containing the NBL table [@holleli1980] has been digitalised. The NBL list is now available as a computer-readable, searchable and manipulable database [@rajeg2023]; this is also available online as a webpage at <https://engganolang.github.io/digitised-holle-list/>. The joining of the translations in the digitalised NBL data into the regional list data is via the matching keys, viz. the index numbers. This digitalised NBL (in a tab-separated plain-text file) has first been implemented in joining the (also digitalised) regional word list for Enggano with the corresponding Dutch, English, and Indonesian glosses in the master Holle List [@rajeg2023eno; @rajeg2025].

Continuing the Enggano research, I envisaged the computational matching between the NBL and all words from the remaining regional languages in the Holle List. To achieve this, the first step is to digitalise the other regional languages from the PDF files into plain texts. Since there are no less than 100 regional lists (comprising of ten volumes) in the Holle List, we need a way to scale-up the digitalisation process.

This paper highlights the use of cloud computing platform, that is “Google Colaboratory” (<https://colab.google/>) [@google2026], to handle the computational resources (such as the Central Processing Unit and Memory) to run large-scale digitalisation process of many PDF files into into plain-text file. The software that performs the remediation from PDF to plain text is the *Tesseract* O(ptical) C(haracter) R(ecognition) engine [@smith2007] (see @sec-data-source-processing for further details). Once the digitalisation output of the regional lists has been checked, edited for errors (cf. @sec-discussion esp. in @tbl-correction-sample), and tagged to separate each language in the source PDF file (@fig-borneo-tag), it is possible to computationally collate what was two-parts paper-borne publications into a digital cross-linguistic lexical database whereby the words in the regional languages are matched with their corresponding Dutch, English, and Indonesian/Malay glosses.

The future potentials of these large lexical data are diverse. It will open new possibilities for systematic computational historical linguistic analysis in finding relationship between languages [@lai2023]. The database can also be used to study diachronic changes of the same language, combining the older data set and (if available) the present-day, modern dataset [@krauße; @rajeg2024]. In the area of lexical semantics, the database could be used to investigate collexification patterns (i.e., cross-language polysemy) [@clics2020; @françois2008] (@tbl-land, @tbl-borneo-land). Last but not least, the database contributes to the preservation and empowerment of Indonesian regional languages, especially the older varieties, in the digital realm, corresponding to UNESCO’s *Digital Initiatives for Indigenous Languages* [@llanes-ortiz2023].

## Methodology {#sec-method}

### Data source acquisition {#sec-data-source-acquisition}

The PDF files for all eleven volumes of the Holle List series are available open access on the Open Research Repository of the Australian National University (ANU) library, under the [“ANU Asia-Pacific Linguistics/Pacific Linguistics Press” collection](https://openresearch-repository.anu.edu.au/collections/bbe2244b-212d-44b4-b33a-59048fd7e13f). The Holle List publications can be looked up using "Holle Lists" as the search term (see @fig-holle-list-search-output).

[![A snippet of the search results for the Holle List publications on the ANU Open Research Repository](img/holle-list-search-results.png){#fig-holle-list-search-output}](https://openresearch-repository.anu.edu.au/search?spc.page=1&query=Holle%20Lists&scope=bbe2244b-212d-44b4-b33a-59048fd7e13f)

The PDF files for every volume was downloaded and uploaded onto the Google Drive of the Holle List project so that they are accessible when processed in the Google Colab coding environment. This is explained next.

### Data processing {#sec-data-source-processing}

To use Google Colab, one only needs to sign up for a Google Account (if they have not had one). After signing-in to one’s Google Account, go to <https://colab.google/> and choose the New Notebook option. A computational Jupyter Notebook will be created and stored in the Google Drive folder. This Notebook runs Python programming language (see @fig-jupyter-notebook). All computations for the digitalisation happened on this online, cloud computer on Google Colab.

![A snippet of an interface of the Jupyter Notebook in Google Colab](img/google-collab-notebook-interface.png){#fig-jupyter-notebook}

The codes shown in @fig-jupyter-notebook are for installing relevant software in the remediation process from PDF into plain text. The *Tesseract OCR* engine (see the code `!sudo apt install tesseract-ocr`) as well as the Python package/module *pytesseract* [@hoffstaetter2024] were installed to allow access to *Tesseract* via Python. Before converting the PDF into plain text with the *pytesseract*, the PDF files need to be converted into images using the `pdf2image` module [@belval2024].

The next step is to load the necessary functionality from the installed modules for PDF-to-text conversion, including a function that allows accessing the downloaded PDFs stored on Google Drive (cf. @fig-holle-list-search-output). This is shown in @fig-module-load.

![Loading the relevant functions from the installed modules](img/google-colab-module-load.png){#fig-module-load}

After loading the relevant functions by executing codes in @fig-module-load, we need to write custom Python codes for the digitalisation processes. An example is shown in @fig-python-ocr for the processing of the regional Holle List vol. 5/1 for the Papuan and Austronesian languages in the Digul Area, Irian Jaya/West Papua, Indonesia [@stokhof1982].

![Custom Python codes for the digitalisation of the PDF into plain text](img/google-colab-digitalisation.png){#fig-python-ocr}

Codes in the upper code block/box in @fig-python-ocr deal with converting the PDF into image by providing the Google Drive path of the PDF. After that come the codes in the lower code block/box in @fig-python-ocr. They cover three aspects:

1.  Setting-up the parameters or configuration for the output format of the plain text. Details can be found at <https://tesseract-ocr.github.io/tessdoc/Command-Line-Usage.html>
2.  Creating the main processing iteration to convert the images into a single text file. This is shown from the code line containing `for i, image in enumerate(images):` up until the line stating `full_text += "\n\n"`.
3.  Saving the output into a plain text file; this file is stored on a Google Drive folder chosen by the user. The code line shown in @fig-python-ocr indicates that the output is saved with the file name `HolleList-Vol-5-1-Irian-Jaya.txt` under a folder for the Vol. 5/1, which is in turn inside the `hollelist` sub-folder in my main Google Drive folder (`MyDrive`) (see @fig-ocr-output).

![A Google Drive folder containing the plain-text (.txt) output of the OCR operation for the PDF of the Holle List vol. 5/1](img/digitalisation-output.png){#fig-ocr-output}

Note that writing the codes as presented in @fig-python-ocr does not necessarily mean that the codes are automatically executed/instructed to produce the output. To run the codes inside a given code block, hover the mouse in the top-left area of the code block until a white rightward arrow (with black background) appears (see the point of the blue arrow in @fig-executing-code), then click on that white arrow.

![The execution of the codes using the graphical user interface button or keyboard shortcut](img/google-colab-running-code.png){#fig-executing-code}

Alternatively, ensure the cursor is in the relevant code block and then use the keyboard shortcut `Ctrl+Enter` to execute the codes in that code block.

As mentioned in @sec-intro, the execution of these codes is not performed locally on our laptop but online on the cloud, using the Central Processing Unit (CPU) and the memory provided by the Google Colab. It is also important to note that the conversion processes from image files into plain texts for a given volume could take more than one hour. In this study, I did not take note on the processing times for digitalising each volume because the primary goal is not to assess the efficiency and processing times.

## Results {#sec-results}

In this section, I present two sets of results from a single volume, namely the Holle List vol. 5/1 [@stokhof1982] for Austronesian and Papuan languages in the Digul area, Irian Jaya (West Papua). They are chosen to illustrate the contrast of the quality of the conversion output, depending on the nature of the input-characters in the source PDF file. These two sets represent two different languages in that volume, namely Numfor [@stokhof1982, 17] and Digul Mappi (spoken in the area between the Digul river and the Mappi river) [@stokhof1982, 133].

@fig-numfor-1 captures several words in Numfor in the PDF source file while @fig-numfor-2 shows their corresponding OCR conversion output in plain-text.

```{r}
#| label: fig-numfor
#| fig-cap: "Snippets of the PDF source for the Numfor list and its corresponding plain-text format after the image-to-text conversion"
#| fig-subcap: 
#|   - "the PDF source [@stokhof1982, 17]"
#|   - "the plain-text output"
#| fig-cap-location: bottom
#| out-width: "80%"
#| layout-nrow: 2

knitr::include_graphics("img/numfor-pdf.png")
knitr::include_graphics("img/numfor-txt.png")
```

The pair in @fig-numfor can now be contrasted with that for the Digul Mappi list in @fig-digulmappi.

```{r}
#| label: fig-digulmappi
#| fig-cap: "Snippets of the PDF source for the Digul Mappi list and its corresponding plain-text format after the image-to-text conversion"
#| fig-subcap: 
#|   - "the PDF source [@stokhof1982, 133]"
#|   - "the plain-text output"
#| fig-cap-location: bottom
#| out-width: "80%"
#| layout-nrow: 2
#| echo: hide

knitr::include_graphics("img/digul-mappi-pdf.png")
knitr::include_graphics("img/digul-mappi-txt.png")
```

In @sec-discussion below, I discuss these results and relate them with a recent data publication of the Holle List when the master NBL has been joined manually with the regional words into a digital database.

## Discussion {#sec-discussion}

Careful inspection of the result snippets in @fig-numfor and @fig-digulmappi reveal different quality of the output. The words shown for Numfor are normal alphabet without diacritics. The OCR output for these words appears correct and resembles the source PDF without the need to edit them. In addition, the index numbers are rendered correctly.

Let us now contrast that result with that for Digul Mappi in @fig-digulmappi. The Digula Mappi words contain accented characters (i.e., those with diacritics). For example, the word for ‘body’ (ID no. 1) is *wòssò* in Digul Mappi. The OCR engine did not render or convert the character `ò` correctly. Hence, from *wòssò* (original) into *wdssd* in the OCR output (@fig-digulmappi-2). A quick look at some other words containing *ò* in the list in @fig-digulmappi-1 suggests that these combined characters (o + ◌̀) are rendered into *d*:

```{r}
#| label: tbl-sample-error
#| tbl-cap: "OCR rendering output of some words with _ò_ in @fig-digulmappi"
#| tbl-colwidths: [20,80]

tibble::tribble(~`Source PDF`, ~`Holle ID`, ~`OCR output`,
                "_chabãnj**ò**_ ‘hair’", 6, "_chabanj**d**_",
                "_soet**ò**_ ‘ear’", 9, "_soet**d**_",
                "_soet**ò**t**ò**_ ‘earwax’", 10, "_soet**d**t**d**_",
                "_k**ò**koe_ ‘belly’", 54,  "_k**d**koe_",
                "_intaki**ò**_ ‘side’", 60, "_intaki**d**_",
                "_**ò**goe_ ‘navel’", 61, "_**d**goe_"
                
) |> 
  dplyr::relocate(`Holle ID`, .before = `Source PDF`) |> 
  tidyr::separate_wider_regex(cols = `Source PDF`, 
                              patterns = c(`Source PDF` = "^[^ ]+", 
                                           " ", 
                                           Gloss = "[^ ]+")) |> 
  dplyr::relocate(Gloss, .after = `Holle ID`) |> 
  knitr::kable()
```

With such a pattern from the snippet of the data in @tbl-sample-error, we might assume that all other occurrences of *ò* will be rendered as *d*. With that assumption, and to speed-up the fixing, we might also be tempted to perform a global find-and-replace procedure: replacing all occurrences of *d* with *ò*. However, the assumption is not fully confirmed and a one-time find-and-replace procedure is probably not ideal.

Consider another word in @fig-digulmappi-1 referring to ‘navel’ (ID no. 61), namely *mòṙěkiò*. The OCR output of this word shows that the two *ò*-s inside it are rendered differently: as *o* in the first syllable and as *d* in the penultimate syllable. What is more, *d* is not only the OCR rendering for *ò* in the original PDF, but also for the second *à* in the word *àssi**à**běgǐ* (ID no. 67) ‘buttocks’; this word is rendered as *Assi**d**bégi* in the OCR output (@fig-digulmappi-2).

With all these issues, the OCR output from image to text requires further *manual* checking and editing. Nevertheless, computational remediation from image to text on the cloud with *Tesseract* lets us obtain computationally searchable text data relatively quickly, rather than typing manually (cf. below). As we have seen from @fig-numfor, typically the conversion works well for simple characters (i.e., without diacritics or without other formatting, like underscore). This means that we reduce the time and effort to re-type (or manually type) these well-recognised characters, with the focus is only on scanning/checking accented characters[^2].

[^2]: At the moment, all OCR outputs for the Holle List are still in private GitHub repository because the results need manual checking and editing.

Even if we decided that we would administer the digitalisation task as a manual typing work, the result of the typing still needs further manual checking and editing for *human error* and/or inconsistencies. This is what was done for an on-going digitalisation work of the Holle List for the remaining Barrier Islands Languages[^3] (other than Enggano), off the west coast of Sumatra [@rajegDigitisedAnnotatedHolle2025; @holle-list-barrier-islands]. The results from student’s group-project[^4] [@rajeggroupwork] to digitalise these lists as an introduction to *WeSay* were first processed, combined, and then manually checked on Google Spreadsheet for tracking changes [cf. @fomin2006, 84].

[^3]: This work (i) continues a previous digitalisation sub-project of the Enggano Holle List (funded by the Arts and Humanities Research Council \[Grant ID: [AH/W007290/1](https://gtr.ukri.org/projects?ref=AH%2FW007290%2F1){target="_blank"}\] led by the University of Oxford, UK) and (ii) is now part of the Australian Research Council (ARC) research (Grant ID: [DP230102019](https://dataportal.arc.gov.au/NCGP/Web/Grant/Grant/DP230102019){target="_blank"}) on Languages of Barrier Islands in Sumatra, Indonesia (led by the Australian National University in Canberra, Australia).

[^4]: The list of the students’ names contributing to this work and the languages they work on is available at <https://github.com/complexico/lexico-holle-list-barrier-islands?tab=readme-ov-file#student-contributors>. They are also listed as the co-authors for the data publication for each language; access this information at <https://github.com/complexico/holle-list-barrier-islands?tab=readme-ov-file#updates-from-students-contributions>

@tbl-correction-sample shows how tracking changes are organised as a table. The regional lexical items that were manually typed by students (`lx_all` column) are in a separate column from that hosting the correction (called `lx_all_correct`). The `ID` column refers to the Holle List index number and a separate column for its correction is also provided but not shown here.

```{r}
#| label: tbl-correction-sample
#| tbl-cap: "A sample of manually corrected lexical items from the languages of Barrier Islands in the Holle List"

# maindb <- googlesheets4::read_sheet(ss = "https://docs.google.com/spreadsheets/d/1P-JontDvH4MjKZ4pdqxthjTovSajJLKX6y2rtpuO5sc/edit?usp=sharing",
#                                     col_types = c("cccccccccccicccccccccccccccllcccccccccccccc"),
#                                     na = "NA")
# maindb |>
#   dplyr::select(lang_name, ID, lx_all, lx_all_correct) |>
#   dplyr::mutate(is_corrected = ifelse(lx_all_correct == "",
#                                       FALSE,
#                                       TRUE)) |>
#   readr::write_tsv("data/maindb_correction_data.tsv")

# correction_sample <- readr::read_tsv("data/maindb_correction_data.tsv") |>
#   dplyr::filter(!is.na(lx_all_correct)) |>
#   dplyr::filter(!grepl("^add_", ID, perl = TRUE)) |>
#   dplyr::filter(grepl("^\\d+$", ID, perl = TRUE)) |>
#   dplyr::filter(!grepl("^\\d", lx_all, perl = TRUE)) |>
#   dplyr::filter(!grepl("\\s", lx_all_correct, perl = TRUE)) |> 
#   dplyr::select(-is_corrected) |>
#   dplyr::slice_sample(n = 1, by = lang_name)
# correction_sample |>
#   readr::write_tsv("data/correction_sample.tsv")

read.table(file = "data/correction_sample.tsv", header = TRUE, 
           sep = "\t", quote = "", comment.char = "") |> 
  knitr::kable()
```

With that correction set-up, we can (i) compare the original and the edited version, as well as, (ii) for each language, quantify what proportion of all manually typed items receive correction vs. those who are already correct. Such a quantification is visualised in @fig-error-check.

```{r message=FALSE, warning=FALSE}
#| label: fig-error-check
#| fig-cap: "Proportion of lexical items corrected after the manual digitalisation for the Holle List of the Barrier Islands Languages [@holle-list-barrier-islands]."
#| fig-dpi: 600

readr::read_tsv("data/maindb_correction_data.tsv") |> 
  dplyr::count(lang_name, is_corrected) |> 
  dplyr::group_by(lang_name) |> 
  dplyr::mutate(perc = round(n/sum(n) * 100, 1)) |> 
  dplyr::arrange(lang_name, dplyr::desc(n)) |> 
  ggplot2::ggplot(ggplot2::aes(x = lang_name, y = perc,
                               fill = is_corrected)) +
  ggplot2::geom_col() +
  ggplot2::geom_text(ggplot2::aes(label = n),
                     vjust = 1) +
  ggplot2::labs(y = "percentage",
                x = "language label",
                fill = "is corrected?",
                caption = "Raw frequencies are numbers inside the bars")

```

Once the word list has been in digital form and corrected, they can be joined with their translations from the reference Holle List (or the *New Basic List*) (cf. @sec-intro). Then, we can explore and computationally search across a set of languages (e.g., from a given volume of the Holle List) how a certain concept is expressed in these languages. An illustration will be given for languages of the Barrier Islands in Sumatra, Indonesia, whose manual digitalisation work has been nearly completed (pending further work for orthography standardisation and phonemic transliteration). Another example is given for languages of Kalimantan where the *Tesseract* OCR output can be processed computationally which is not possible in PDF format.

```{r message=FALSE, warning=FALSE, echo=FALSE}
#| label: tbl-land
#| tbl-cap: "Forms referring to ‘land’ as a physical landscape (ID 942) and as a state (ID 943) in the Holle List of the Barrier Islands Languages [@holle-list-barrier-islands]"


# set wd to main holle list barrier islands
# setwd("C:/Users/GRajeg/OneDrive - Nexus365/Documents/Research/barrier-island-Holle-list-2023-05-24/")

# wl_files <- dir("C:/Users/GRajeg/OneDrive - Nexus365/Documents/Research/barrier-island-Holle-list-2023-05-24/data-output", pattern = ".tsv", full.names = TRUE)
# 
# lang_name <- str_replace_all(basename(wl_files), "(\\.tsv|_tb)", "")
# 
# db <- map(wl_files, read_tsv) |> 
#   map(~select(., -1)) |> 
#   map2(.y = lang_name, \(x, y) mutate(x, lang_name = y)) |> 
#   map(~mutate(., `v1931` = as.character(`v1931`))) |> 
#   map(~relocate(., lang_name, .before = Index))
# 
# names(db) <- lang_name
# 
# factor_order <- c("lekon", "tapah", "simalur", "seumalur1912", "sigule_salang1912", "salang_sigule1920", "mentawai_nd", "mentawai1933", "nias1905", "nias1911")

enggano <- readr::read_csv("https://raw.githubusercontent.com/engganolang/holle-list-enggano-1895/refs/heads/main/cldf/forms.csv")

enggano2 <- enggano |> 
  select(Index = Holle_ID,
         Forms = Value,
         English, Indonesian) |> 
  mutate(lang_name = "enggano")

# words meaning land and land as a state

# db |> 
#   map(~filter(., list_type == "NBL")) |> 
#   map(~filter(., str_detect(Index, "^94[23]"))) |> 
#   map(~select(., !matches("^nt_"))) |> 
#   list_rbind() |> 
#   select(lang_name, Index, Forms, English) |> 
#   mutate(lang_name = factor(lang_name, levels = factor_order)) |> 
#   arrange(lang_name, Index, Forms) |> 
#   distinct() |> 
#   write_rds("data/land.rds")

db_land <- read_rds("data/land.rds") |> 
  bind_rows(enggano2 |> 
              filter(str_detect(Index, "^94[23]")) |> 
              select(-Indonesian))
db_land |> 
  knitr::kable()

# return
# setwd("C:/Users/GRajeg/OneDrive - Nexus365/Documents/Research/snbi2026-holle-list")
```

@tbl-land illustrates the joint database from the Barrier Islands Languages in the Holle List, filtered specifically for word-forms expressing the concept of LAND literally (ID 942) and metaphorically as a state (ID 943). It can be seen, for example, that Lekon, Tapah, and Simalur colexify [@françois2008, 170; @francoisLexicalTectonicsMapping2022, 95] the concept of LAND as a physical landscape and as a state with a single form, namely *angkal*. Then, the two Sigule and Salang lists from two different periods of collection and publications also colexify these two concepts using two different forms in these two periods. Other languages, such as Nias, mainly distinguish between land as a physical landscape and land as a state. But, an interesting observation can be made about Nias as well. While in Nias 1905 data the form *tano̠* as well as its 1911 given form *tanô*, only refers to land literally (ID 942), the same form in 1911 (*tanô*) can also refer to land as a state; this might suggest a semantic extension of cognates for land as a physical landscape in diachronic varieties of Nias.

The OCR output in plain-text format from running the *Tesseract* engine (@sec-data-source-processing) can also be computationally (via programmatic coding) processed and searched for certain word forms. Before doing this, the output needs to be manually tagged for the language boundary in the text (see the yellow-highlighted line 120 in @fig-borneo-tag). That is, which part of the output belong to which language in the original PDF so that computationally we can write code to detect which form belongs to which language.

```{r}
#| label: fig-borneo-tag
#| fig-cap: "Snippet of grouping per-language word list using XML tag in the OCR output of a volume"
#| echo: false
#| out-width: "80%"

knitr::include_graphics("img/borneo-lang-tag-01.png")

```

Once all language group in a volume has been tagged as in @fig-borneo-tag, a programmatic script in R or Python can be designed to process and access the text file. @tbl-borneo-land shows the tabular output of extracting (with programmatic edit of the OCR output) word forms referring to land as a landscape (ID 942) and/or as a state (ID 943) in the Holle List for languages of Borneo/Kalimantan [@stokhofHolleListsVocabularies1986].

```{r}
#| label: tbl-borneo-land
#| tbl-cap: "Forms referring to ‘land’ as a physical landscape (ID 942) and as a state (ID 943) in the Holle List of the languages of Kalimantan (Borneo) vol. 8 [@stokhofHolleListsVocabularies1986]"

# read_lines("https://raw.githubusercontent.com/Holle-List/holle-list-vol8-kalimantan/refs/heads/main/raw/PL-D69_Holle_List_Vol_8_Kalimantan-Borneo.txt") |> 
#   write_lines("data/vol8.txt")

# borneo <- read_lines("data/vol8.txt")
# grp_start <- str_which(borneo, "\\<group xml\\:lang")
# grp_end <- str_which(borneo, "\\<\\/group\\>")
# borneo_split <- map2(grp_start, grp_end, \(x, y) borneo[x:y])
# names(borneo_split) <- borneo_split |> map_chr(`[`, 1) |> str_extract("(?<=xml\\:lang\\=\").+(?=\" xml\\:id)")
# 
# borneo_split |> 
#   map(~str_subset(., "\\b942|\\b943")) |> 
#   write_rds("data/borneo_split.rds")


borneo_land <- read_rds("data/borneo_split.rds") |> 
  map(~str_replace(., "943. 1€p6'", "943. lěpo̊'")) |> 
  map(~str_replace(., "942\\/", "942\\/943. tanah")) |> 
  map(~str_replace(., "^943\\. tanah$", "")) |> 
  map(~str_subset(., "\\b942|\\b943")) |> 
  map(~str_extract(., "(\\b942\\/943\\b.\\s[^ ]+?(\\b| )|\\b94[23]\\b.\\s[^ ]+?(\\b| ))")) |> 
  unlist()

borneo_no_land <- read_rds("data/borneo_split.rds") |> 
    map(~str_replace(., "943. 1€p6'", "943. lěpo̊'")) |> 
    map(~str_replace(., "942\\/", "942\\/943. tanah")) |> 
    map(~str_replace(., "^943\\. tanah$", "")) |> 
    map(~str_subset(., "\\b942|\\b943")) |> 
    map(~str_extract(., "(\\b942\\/943\\b.\\s[^ ]+?(\\b| )|\\b94[23]\\b.\\s[^ ]+?(\\b| ))")) |> 
  map(\(x) x[identical(x, character(0))]) |> 
  unlist() |> 
  names() |> 
  str_to_title()

borneo_land_to_print <- tibble(lang_name = names(borneo_land), Forms = borneo_land) |> 
  mutate(lang_name = str_to_title(lang_name)) |> 
  mutate(lang_name = str_replace(lang_name, "\\d$", "")) |> 
  mutate(Forms = if_else(lang_name == "Katingan Dayak", str_replace(Forms, "^943\\. p.tak", "942/943. pètak"), Forms)) |> 
  bind_rows(tibble(lang_name = "Kenyah Dayak", Forms = "942. tǎnà'")) |> 
  separate_wider_regex(Forms, c(Index = "^[^ .]+", "\\. ", Forms = ".+"))

additional_row <- borneo_land_to_print |> 
  slice(nrow(borneo_land_to_print))
row_1_tgt <- borneo_land_to_print |> 
  slice(1:7)
row_rest <- borneo_land_to_print |> 
  slice(8:10)
borneo_land_to_print <- bind_rows(row_1_tgt,
                                  additional_row,
                                  row_rest)

borneo_land_to_print |> 
  # arrange(lang_name, Forms) |> 
  knitr::kable()
```

It is important to mention that not all languages/varieties represented in the Holle List vol. 8 for Kalimantan contain forms referring to the concept of LAND in ID 942 and 943 (i.e., our programmatic search returned null results for these varieties with respect to these two IDs). These varieties are `r borneo_no_land |> unique() |> str_c(collapse = ", ")`. The reasons why they are not attested need further investigation.

## Conclusion

The aim of the work reported in this paper is to scale up the digitalisation of the many volumes of the Holle List into computer searchable and manipulateable dataset. This paper discusses computational tool and resource used to achieve that aim (XREF). We have shown issues arising from the OCR output (XREF) and the need to handle that manually, even when the digitalisation was performed manually from the start (XREF). We hope to have provided simple illustration of the potential of computational and linguistic queries and further processings on the database once they are ready in computer-searchable, digital format.

We need to keep in mind that the regional language data represent the state of the language in the late 19th century and/or early 20th century. Other things to keep in mind is that there can be variation of orthography/spelling between investigators of different

The aim here is to demonstrate how a paper-based, disjointed set of information relevant for the Humanities, especially Indigenous language preservation, can be re-conceptualised and re-mediated digitally for further uses.

## References
